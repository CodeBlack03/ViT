{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb077da4",
   "metadata": {},
   "source": [
    "In this question, you will implement a **vision transformer** based image classification model using pytorch.\n",
    "a)Implement a basic version of **vision transformer(https://arxiv.org/pdf/2010.11929.pdf)**, that first divides an image into **patches** and then passes them through a set of **multihead self attention** modules to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc7d419d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc7d419d",
    "outputId": "a0996d45-d911-4400-c341-8e24bf46f921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Requirement already satisfied: torch in /home/vharsh/anaconda3/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in /home/vharsh/anaconda3/lib/python3.9/site-packages (0.15.1)\n",
      "Requirement already satisfied: torchaudio in /home/vharsh/anaconda3/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: jinja2 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: filelock in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: networkx in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: wheel in /home/vharsh/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/vharsh/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (63.4.1)\n",
      "Requirement already satisfied: cmake in /home/vharsh/anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.26.0)\n",
      "Requirement already satisfied: lit in /home/vharsh/anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: numpy in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/vharsh/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "torch version: 2.0.0+cu117\n",
      "torchvision version: 0.15.1+cu117\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60a89734",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60a89734",
    "outputId": "5fde6899-b167-4640-dccb-01029de63d2e"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d4f15aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3d4f15aa",
    "outputId": "0f12d8b0-6f33-466f-ccfe-5255dfeccda4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337232b",
   "metadata": {
    "id": "8337232b"
   },
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40e43284",
   "metadata": {
    "id": "40e43284"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "284c4697",
   "metadata": {
    "id": "284c4697"
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10cbf8",
   "metadata": {
    "id": "ea10cbf8"
   },
   "source": [
    "# preparing transforms and dataloader for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f62a9524",
   "metadata": {
    "id": "f62a9524"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Hyper parameter\n",
    "LEARNING_RATE=10**(-4)\n",
    "BATCH_SIZE = 100 \n",
    "IMG_SIZE = 32\n",
    "PATCH_SIZE=4\n",
    "IN_CHANNELS=3\n",
    "EMBEDDING_DIMENTION=PATCH_SIZE*PATCH_SIZE*IN_CHANNELS*5\n",
    "NUM_HEADS=12\n",
    "NUM_ATTENTION_LAYERS=4\n",
    "DROPOUT_P=0.1\n",
    "MLP_SIZE=1024\n",
    "NUM_LABELS=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12ea1640",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192,
     "referenced_widgets": [
      "a96f8e1477f2497a8fce5dd8be923415",
      "f195c48d67144d4591a94a700f99abcd",
      "67466fcb2a8e4836a8a2e5760cde277a",
      "b76bff5e97db44069aeb0c22358721c4",
      "41012c8b07a74a2eb110a75fae49246a",
      "314ceb5ac10a4abcbac750b5efd6f1a8",
      "6728a139fd9b4e7c9a5e1adf41bfa33d",
      "763028a4d9d14e988724aa003120362e",
      "4880fff204524b73b92a64694825abd2",
      "100a1d795a804d7a87296be75151fc5a",
      "fdbbe0a2fc764113afc69a1e9da3b284"
     ]
    },
    "id": "12ea1640",
    "outputId": "1587b50f-831c-4093-8333-2f22b1eb01b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually created transforms: Compose(\n",
      "    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "    ToTensor()\n",
      ")\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f4838181100>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f4838181280>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])           \n",
    "print(f\"Manually created transforms: {manual_transforms}\")\n",
    "\n",
    "\n",
    "#imgdata contains transformed images\n",
    "train_dataset = CIFAR10(root='imgdata/', download=True, transform=manual_transforms)\n",
    "test_dataset = CIFAR10(root='imgdata/', train=False, transform=manual_transforms)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      shuffle=False,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "test_dataloader = DataLoader(\n",
    "      test_dataset,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      shuffle=False,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09d2954a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "09d2954a",
    "outputId": "ab54caa2-5a3b-455e-95b1-dbfbb45bdc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 32, 32]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch of images\n",
    "\n",
    "\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "print(image_batch.shape,label_batch.shape)\n",
    "# Get a single image from the batch\n",
    "image, label = image_batch[12], label_batch[12]\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(train_dataset.classes[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c1c3a",
   "metadata": {
    "id": "402c1c3a"
   },
   "source": [
    "# Patch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15439577",
   "metadata": {
    "id": "15439577"
   },
   "outputs": [],
   "source": [
    "class Patch_embedding(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 patch_size=8,\n",
    "                 embedding_dim=768\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.patcher=nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=embedding_dim,\n",
    "                               kernel_size=patch_size,\n",
    "                               stride=patch_size,\n",
    "                               padding=0\n",
    "                              )\n",
    "        \n",
    "        self.flatten = nn.Flatten(start_dim=2,\n",
    "                                  end_dim=3)\n",
    "    def forward(self,x):\n",
    "        image_resolution = x.shape[-1]\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched)\n",
    "        \n",
    "        return x_flattened.permute(0, 2, 1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5d22d41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5d22d41",
    "outputId": "06e66731-d44c-40ca-da29-25a7d4454fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: torch.Size([1, 3, 32, 32])\n",
      "Output patch embedding shape: torch.Size([1, 64, 240])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "patchify=Patch_embedding(in_channels=IN_CHANNELS,\n",
    "                 patch_size=PATCH_SIZE,\n",
    "                 embedding_dim=EMBEDDING_DIMENTION)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) \n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b401aa1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b401aa1e",
    "outputId": "1a418a42-dc18-4b3f-9216-27a713ba4f66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Patch_embedding                          --\n",
       "├─Conv2d: 1-1                            148,224\n",
       "├─Flatten: 1-2                           --\n",
       "=================================================================\n",
       "Total params: 148,224\n",
       "Trainable params: 148,224\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Patch_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da7526",
   "metadata": {
    "id": "19da7526"
   },
   "source": [
    "## including class token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a34a289d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a34a289d",
    "outputId": "c042d874-9720-4cd6-f7b1-7d901738c9ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 240])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_token= nn.Parameter(torch.ones((1,1,EMBEDDING_DIMENTION)),requires_grad=True) #1 can be replaced with BATCH_SIZE\n",
    "class_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf7a4265",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf7a4265",
    "outputId": "9ef4433f-1d18-48ba-de44-0163d211b7b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 240])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embedding=torch.cat((class_token,patch_embedded_image),dim=1)\n",
    "final_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7118fe5",
   "metadata": {
    "id": "e7118fe5"
   },
   "source": [
    "# MSA LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9ec36ff",
   "metadata": {
    "id": "c9ec36ff"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embedding_dim =EMBEDDING_DIMENTION, \n",
    "                 num_heads=NUM_HEADS, \n",
    "                 attn_dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x,\n",
    "                                             key=x, \n",
    "                                             value=x, \n",
    "                                             need_weights=False)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8dbe3b1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dbe3b1a",
    "outputId": "f5cb3dd4-512f-40c6-fe5b-15079a72c1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 65, 240])\n",
      "torch.Size([1, 65, 240])\n"
     ]
    }
   ],
   "source": [
    "Multihead=MultiheadSelfAttentionBlock()\n",
    "print(final_embedding.shape)\n",
    "after_msa_out=Multihead(final_embedding)\n",
    "print(after_msa_out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe71e62",
   "metadata": {
    "id": "dfe71e62"
   },
   "source": [
    "# MLP layer in encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "394b11a3",
   "metadata": {
    "id": "394b11a3"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embedding_dim=EMBEDDING_DIMENTION, \n",
    "                 mlp_size = MLP_SIZE, \n",
    "                 dropout = DROPOUT_P): \n",
    "        super().__init__()\n",
    "        \n",
    "      \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "       \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(), \n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, \n",
    "                      out_features=embedding_dim), \n",
    "            nn.Dropout(p=dropout) \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6247ae96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6247ae96",
    "outputId": "53a01c48-0319-49d1-edda-9eee144b1415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 65, 240])\n",
      "torch.Size([1, 65, 240])\n"
     ]
    }
   ],
   "source": [
    "MLP=MLPBlock()\n",
    "print(after_msa_out.shape)\n",
    "after_mlp_out=MLP(after_msa_out)\n",
    "print(after_mlp_out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fde9c",
   "metadata": {
    "id": "998fde9c"
   },
   "source": [
    "# ENCODER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cab14a03",
   "metadata": {
    "id": "cab14a03"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "   \n",
    "    def __init__(self,\n",
    "                 embedding_dim=EMBEDDING_DIMENTION, \n",
    "                 num_heads=NUM_HEADS, \n",
    "                 mlp_size=MLP_SIZE, \n",
    "                 mlp_dropout=DROPOUT_P, \n",
    "                 attn_dropout=0): \n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        \n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x =  self.msa_block(x) + x \n",
    "        \n",
    "        \n",
    "        x = self.mlp_block(x) + x \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2b88b",
   "metadata": {
    "id": "3ac2b88b"
   },
   "source": [
    "# ViT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc47e507",
   "metadata": {
    "id": "bc47e507"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ViT(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_size=IMG_SIZE, \n",
    "                 in_channels=IN_CHANNELS, \n",
    "                 patch_size=PATCH_SIZE, \n",
    "                 num_transformer_layers=NUM_ATTENTION_LAYERS, \n",
    "                 embedding_dim=EMBEDDING_DIMENTION, \n",
    "                 mlp_size=MLP_SIZE, \n",
    "                 num_heads=NUM_HEADS, \n",
    "                 attn_dropout=0, \n",
    "                 mlp_dropout=DROPOUT_P, \n",
    "                 embedding_dropout=DROPOUT_P, # Dropout for patch and position embeddings\n",
    "                 num_classes=NUM_LABELS): \n",
    "        \n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "                 \n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        \n",
    "        \n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "       \n",
    "        self.patch_embedding = Patch_embedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "        \n",
    "        \n",
    "        self.transformer_encoder = nn.Sequential(*[EncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "       \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = BATCH_SIZE\n",
    "        \n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) \n",
    "        \n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = self.classifier(x[:, 0]) \n",
    "\n",
    "        return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "tCxOSxu7NO0e",
   "metadata": {
    "id": "tCxOSxu7NO0e"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model,\n",
    "          train_dataloader,\n",
    "          test_dataloader,\n",
    "          max_epochs,\n",
    "          device,\n",
    "         ):\n",
    "    \n",
    "    \n",
    "    optimiser=torch.optim.Adam(params=model.parameters(), \n",
    "                             lr=10**(-4))\n",
    "#                              betas=(0.9, 0.999), \n",
    "#                              weight_decay=0.3) \n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    loss_curve=[]\n",
    "    train_acc_curve=[]\n",
    "    test_acc_curve=[]\n",
    "    \n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        train_accuracy=0\n",
    "        test_acc=0\n",
    "        \n",
    "        for batch_id,(X,y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred=model(X)\n",
    "            #print(y_pred.shape,y.shape,y_pred,y)\n",
    "            loss=loss_fn(y_pred,y)\n",
    "            \n",
    "            train_loss+=loss.item()\n",
    "            \n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            \n",
    "            y_pred=torch.argmax(y_pred,dim=1)\n",
    "            #print(y_pred.shape)\n",
    "            batch_correct=((y_pred==y).sum().item())/BATCH_SIZE\n",
    "            train_accuracy+=batch_correct\n",
    "           # print(loss.item(),batch_correct)\n",
    "        \n",
    "\n",
    "\n",
    "        for batch_id,(X,y) in enumerate(test_dataloader):\n",
    "                  \n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred=model(X)\n",
    "\n",
    "\n",
    "            y_pred=torch.argmax(y_pred,dim=1)\n",
    "\n",
    "            batch_correct=((y_pred==y).sum().item())/BATCH_SIZE\n",
    "            test_acc+=batch_correct\n",
    "          \n",
    "            \n",
    "            \n",
    "        loss_curve.append(train_loss/len(train_dataloader))\n",
    "        train_acc_curve.append(train_accuracy/len(train_dataloader))\n",
    "        test_acc_curve.append(test_acc/len(test_dataloader))\n",
    "        \n",
    "        print(f\"epoch:{epoch+1} train_loss:{loss_curve[-1]} train_acc:{train_acc_curve[-1]} test_acc:{test_acc_curve[-1]}\")\n",
    "             \n",
    "    return loss_curve[-1],train_acc_curve[-1],test_acc_curve[-1]\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bb97b45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bb97b45",
    "outputId": "f5930d9e-4846-4d2c-dc86-3c197f1987fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "ViT                                                               15,840\n",
       "├─Dropout: 1-1                                                    --\n",
       "├─Patch_embedding: 1-2                                            --\n",
       "│    └─Conv2d: 2-1                                                11,760\n",
       "│    └─Flatten: 2-2                                               --\n",
       "├─Sequential: 1-3                                                 --\n",
       "│    └─EncoderBlock: 2-3                                          --\n",
       "│    │    └─MultiheadSelfAttentionBlock: 3-1                      231,840\n",
       "│    │    └─MLPBlock: 3-2                                         493,264\n",
       "│    └─EncoderBlock: 2-4                                          --\n",
       "│    │    └─MultiheadSelfAttentionBlock: 3-3                      231,840\n",
       "│    │    └─MLPBlock: 3-4                                         493,264\n",
       "│    └─EncoderBlock: 2-5                                          --\n",
       "│    │    └─MultiheadSelfAttentionBlock: 3-5                      231,840\n",
       "│    │    └─MLPBlock: 3-6                                         493,264\n",
       "│    └─EncoderBlock: 2-6                                          --\n",
       "│    │    └─MultiheadSelfAttentionBlock: 3-7                      231,840\n",
       "│    │    └─MLPBlock: 3-8                                         493,264\n",
       "├─Sequential: 1-4                                                 --\n",
       "│    └─LayerNorm: 2-7                                             480\n",
       "│    └─Linear: 2-8                                                2,410\n",
       "==========================================================================================\n",
       "Total params: 2,930,906\n",
       "Trainable params: 2,930,906\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ViT())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794656c7",
   "metadata": {
    "id": "jvp7QquiNP-J"
   },
   "source": [
    "## Patch_Size_Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cb733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Size = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▎                                      | 1/10 [01:50<16:38, 110.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 train_loss:1.9844052996635437 train_acc:0.26818000000000014 test_acc:0.35700000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▌                                  | 2/10 [03:50<15:26, 115.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 train_loss:1.7382736122608184 train_acc:0.37383999999999984 test_acc:0.40840000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████▉                              | 3/10 [05:45<13:28, 115.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 train_loss:1.6128592882156372 train_acc:0.42121999999999976 test_acc:0.4414000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▏                         | 4/10 [07:45<11:44, 117.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 train_loss:1.5281915171146392 train_acc:0.45112000000000035 test_acc:0.46329999999999993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████▌                     | 5/10 [09:48<09:57, 119.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 train_loss:1.4615172436237336 train_acc:0.4743599999999998 test_acc:0.47619999999999985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████▊                 | 6/10 [11:49<07:59, 119.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 train_loss:1.4148858723640443 train_acc:0.49083999999999994 test_acc:0.49110000000000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████             | 7/10 [13:50<06:00, 120.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 train_loss:1.3694533975124359 train_acc:0.5062600000000003 test_acc:0.5086000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████▍        | 8/10 [15:50<04:00, 120.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 train_loss:1.3340743072032928 train_acc:0.5186000000000004 test_acc:0.5132000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████▋    | 9/10 [17:51<02:00, 120.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 train_loss:1.2979206187725068 train_acc:0.5321800000000007 test_acc:0.5296000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [19:51<00:00, 119.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 train_loss:1.264866498351097 train_acc:0.5466400000000008 test_acc:0.5345000000000002\n",
      "Patch Size = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▎                                      | 1/10 [02:03<18:28, 123.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 train_loss:1.9975338139533996 train_acc:0.25967999999999974 test_acc:0.35279999999999995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▌                                  | 2/10 [04:05<16:21, 122.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 train_loss:1.7329924674034118 train_acc:0.37739999999999996 test_acc:0.4050000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████▉                              | 3/10 [06:05<14:10, 121.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 train_loss:1.6217646906375884 train_acc:0.41817999999999955 test_acc:0.43690000000000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▏                         | 4/10 [08:04<12:03, 120.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 train_loss:1.5385855824947356 train_acc:0.4496599999999997 test_acc:0.4603999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████▌                     | 5/10 [10:06<10:04, 120.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 train_loss:1.4784809691905976 train_acc:0.46865999999999963 test_acc:0.47729999999999984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████▊                 | 6/10 [12:10<08:08, 122.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 train_loss:1.4258230352401733 train_acc:0.4889399999999995 test_acc:0.48540000000000005\n"
     ]
    }
   ],
   "source": [
    "Patches=[2,4,8,16,32]\n",
    "loss_per_patch=[]\n",
    "train_acc_per_patch=[]\n",
    "test_acc_per_patch=[]\n",
    "for i in range(5):\n",
    "    PATCH_SIZE = Patches[i]\n",
    "    print(f\"Patch Size = {PATCH_SIZE}\")\n",
    "    vit=ViT()\n",
    "    loss,train_accuracy,test_accuracy=train(model=vit,\n",
    "      train_dataloader=train_dataloader,\n",
    "      test_dataloader=test_dataloader,\n",
    "      device=device,\n",
    "      max_epochs=10,\n",
    "      )\n",
    "    train_acc_per_patch.append(train_accuracy)\n",
    "    test_acc_per_patch.append(test_accuracy)\n",
    "    loss_per_patch.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f781bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_acc_per_patch)\n",
    "# print(test_acc_per_patch)\n",
    "# #plt.plot()\n",
    "# plt.figure(figsize=(15,6))\n",
    "\n",
    "# plt.plot(Patches,train_acc_per_patch,label=\"accuracy\")\n",
    "# #plt.plot(Patches,test_acc_per_patch,label=\"accuracy\")\n",
    "# plt.xlabel(\"PATCHES\")\n",
    "# plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add00057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot()\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(Patches,train_acc_per_patch,label=\"train-accuracy\")\n",
    "plt.plot(Patches,test_acc_per_patch,label=\"test-accuracy\")\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"PATCHES\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc32745a",
   "metadata": {},
   "source": [
    "## ATTENTION HEADS EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df42818",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention_Heads=[4,8,12,16]\n",
    "loss_per_patch=[]\n",
    "train_acc_per_patch=[]\n",
    "test_acc_per_patch=[]\n",
    "for i in range(4):\n",
    "    NUM_HEADS = Attention_Heads[i]\n",
    "    print(f\"Attention Heads = {NUM_HEADS}\")\n",
    "    vit=ViT()\n",
    "    loss,train_accuracy,test_accuracy=train(model=vit,\n",
    "      train_dataloader=train_dataloader,\n",
    "      test_dataloader=test_dataloader,\n",
    "      device=device,\n",
    "      max_epochs=10,\n",
    "      )\n",
    "    train_acc_per_patch.append(train_accuracy)\n",
    "    test_acc_per_patch.append(test_accuracy)\n",
    "    loss_per_patch.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eee94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc_per_patch)\n",
    "print(test_acc_per_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0cb591",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(Attention_Heads,train_acc_per_patch,label=\"train-accuracy\")\n",
    "plt.plot(Attention_Heads,test_acc_per_patch,label=\"test-accuracy\")\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"NUM_HEADS\")\n",
    "plt.ylabel(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de9d12",
   "metadata": {},
   "source": [
    "No change in the accuracy as the image size is really low, as that was proposed in paper which is 224x224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6u9xgTrNUBh",
   "metadata": {
    "id": "d6u9xgTrNUBh"
   },
   "source": [
    "## ViT - every level feedback (EXPREIMENT 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z0e2Mv7XNQFu",
   "metadata": {
    "id": "z0e2Mv7XNQFu"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ViT(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_size=IMG_SIZE, \n",
    "                 in_channels=IN_CHANNELS, \n",
    "                 patch_size=PATCH_SIZE, \n",
    "                 num_transformer_layers=NUM_ATTENTION_LAYERS, \n",
    "                 embedding_dim=EMBEDDING_DIMENTION, \n",
    "                 mlp_size=MLP_SIZE, \n",
    "                 num_heads=NUM_HEADS, \n",
    "                 attn_dropout=0, \n",
    "                 mlp_dropout=DROPOUT_P, \n",
    "                 embedding_dropout=DROPOUT_P, # Dropout for patch and position embeddings\n",
    "                 num_classes=NUM_LABELS): \n",
    "        \n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "                 \n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        \n",
    "        \n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "       \n",
    "        self.patch_embedding = Patch_embedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "        \n",
    "        \n",
    "        self.EncoderBlock1 = EncoderBlock(embedding_dim=embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        mlp_size=mlp_size,\n",
    "                                        mlp_dropout=mlp_dropout)\n",
    "        self.EncoderBlock2 = EncoderBlock(embedding_dim=embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        mlp_size=mlp_size,\n",
    "                                        mlp_dropout=mlp_dropout)\n",
    "        self.EncoderBlock3 = EncoderBlock(embedding_dim=embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        mlp_size=mlp_size,\n",
    "                                        mlp_dropout=mlp_dropout)\n",
    "        self.EncoderBlock4 = EncoderBlock(embedding_dim=embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        mlp_size=mlp_size,\n",
    "                                        mlp_dropout=mlp_dropout)\n",
    "       \n",
    "       \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        T=torch.zeros(4,BATCH_SIZE,NUM_LABELS)\n",
    "        batch_size = BATCH_SIZE\n",
    "        \n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) \n",
    "        \n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.EncoderBlock1(x)\n",
    "        T[0]=self.classifier(x[:, 0])\n",
    "        x = self.EncoderBlock2(x)\n",
    "        T[1]=self.classifier(x[:, 0])\n",
    "        x = self.EncoderBlock3(x)\n",
    "        T[2]=self.classifier(x[:, 0])\n",
    "        x = self.EncoderBlock4(x)\n",
    "        T[3]=self.classifier(x[:, 0])\n",
    "        #print(T)\n",
    "        #print(\"hello\")\n",
    "        return(T)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2x8ibuZpNQNk",
   "metadata": {
    "id": "2x8ibuZpNQNk"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model,\n",
    "          train_dataloader,\n",
    "          test_dataloader,\n",
    "          max_epochs,\n",
    "          device,\n",
    "         ):\n",
    "    \n",
    "    \n",
    "    optimiser=torch.optim.Adam(params=model.parameters(), \n",
    "                             lr=10**(-4))\n",
    "#                              betas=(0.9, 0.999), \n",
    "#                              weight_decay=0.3) \n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    train_accuracy={0:0,1:0,2:0,3:0}\n",
    "    test_accuracy={0:0,1:0,2:0,3:0}\n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        model.train()\n",
    "        train_accuracy={0:0,1:0,2:0,3:0}\n",
    "        test_accuracy={0:0,1:0,2:0,3:0}\n",
    "        \n",
    "        for batch_id,(X,y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred=model(X)\n",
    "            y_pred=y_pred.to(device)\n",
    "            #print(y_pred.shape,y.shape,y_pred.device,y.device)\n",
    "\n",
    "            loss={}\n",
    "            loss[0]=loss_fn(y_pred[0],y)\n",
    "            loss[1]=loss_fn(y_pred[1],y)\n",
    "            loss[2]=loss_fn(y_pred[2],y)\n",
    "            loss[3]=loss_fn(y_pred[3],y)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss[3].backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_correct={}\n",
    "            batch_correct[0]=((torch.argmax(y_pred[0],dim=1)==y).sum().item())/BATCH_SIZE\n",
    "            batch_correct[1]=((torch.argmax(y_pred[1],dim=1)==y).sum().item())/BATCH_SIZE\n",
    "            batch_correct[2]=((torch.argmax(y_pred[2],dim=1)==y).sum().item())/BATCH_SIZE\n",
    "            batch_correct[3]=((torch.argmax(y_pred[3],dim=1)==y).sum().item())/BATCH_SIZE\n",
    "\n",
    "            train_accuracy[0]+=batch_correct[0]\n",
    "            train_accuracy[1]+=batch_correct[1]\n",
    "            train_accuracy[2]+=batch_correct[2]\n",
    "            train_accuracy[3]+=batch_correct[3]\n",
    "           # print(loss.item(),batch_correct)\n",
    "        \n",
    "\n",
    "\n",
    "        for batch_id,(X,y) in enumerate(test_dataloader):\n",
    "                  \n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred=model(X)\n",
    "            y_pred=y_pred.to(device)\n",
    "            \n",
    "          \n",
    "            batch_correct={}\n",
    "            batch_correct[0]=((torch.argmax(y_pred[0],dim=1)==y).sum().item())/BATCH_SIZE\n",
    "            batch_correct[1]=((torch.argmax(y_pred[1],dim=1)==y).sum().item())/BATCH_SIZE\n",
    "            batch_correct[2]=((torch.argmax(y_pred[2],dim=1)==y).sum().item())/BATCH_SIZE\n",
    "            batch_correct[3]=((torch.argmax(y_pred[3],dim=1)==y).sum().item())/BATCH_SIZE\n",
    "\n",
    "            test_accuracy[0]+=batch_correct[0]\n",
    "            test_accuracy[1]+=batch_correct[1]\n",
    "            test_accuracy[2]+=batch_correct[2]\n",
    "            test_accuracy[3]+=batch_correct[3]\n",
    "            \n",
    "              \n",
    "              \n",
    "    \n",
    "    \n",
    "    print(\"done\")\n",
    "    return train_accuracy,test_accuracy\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169a107",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b169a107",
    "outputId": "56501f67-a082-442d-cda2-f514d29ec782"
   },
   "outputs": [],
   "source": [
    "vit=ViT()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tWsp7XipQmIP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWsp7XipQmIP",
    "outputId": "b491a2c1-9111-4d13-a61b-1bba3de24caa"
   },
   "outputs": [],
   "source": [
    "train_accuracy,test_accuracy=train(model=vit,\n",
    "      train_dataloader=train_dataloader,\n",
    "      test_dataloader=test_dataloader,\n",
    "      device=device,\n",
    "      max_epochs=10,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cGtAP1k3fT3V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGtAP1k3fT3V",
    "outputId": "47977079-0dbb-4ac7-866b-e2fda0ed9555"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array(list(train_accuracy.values()))/len(train_dataloader)\n",
    "b=np.array(list(test_accuracy.values()))/len(test_dataloader)\n",
    "a,b\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot([1,2,3,4],a,marker='o',label=\"train_accuracy\")\n",
    "plt.plot([1,2,3,4],b,marker='o',label=\"test_accuracy\")\n",
    "plt.xlabel(\"ith Layer class token\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OBilwpKocoCo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBilwpKocoCo",
    "outputId": "6b3b1914-d910-4e0b-8abd-32b350dcfea8"
   },
   "outputs": [],
   "source": [
    "X=torch.rand(2,3)\n",
    "X.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RG9uCTTZs-KU",
   "metadata": {
    "id": "RG9uCTTZs-KU"
   },
   "source": [
    "#EXPERIMENT - ATTENTION HEADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z7ETZuRHs8pL",
   "metadata": {
    "id": "z7ETZuRHs8pL"
   },
   "outputs": [],
   "source": [
    "DIFF_NUM_HEADS=[4,6,8,10,12,16]\n",
    "\n",
    "num_head_map={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10a056",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e10a056",
    "outputId": "84035f5f-74d5-415e-8c28-b291233ef904"
   },
   "outputs": [],
   "source": [
    "for NUM_HEADS in DIFF_NUM_HEADS:\n",
    "    vit=ViT()\n",
    "    loss_curve,train_acc_curve,test_acc_curve=train(model=vit,\n",
    "      train_dataloader=train_dataloader,\n",
    "      test_dataloader=test_dataloader,\n",
    "      device=device,\n",
    "      max_epochs=5,\n",
    "      )\n",
    "    num_head_map[NUM_HEADS]=[loss_curve[-1],train_acc_curve[-1],test_acc_curve[-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YWcEFBuuzZ2A",
   "metadata": {
    "id": "YWcEFBuuzZ2A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "P54qdRGoRgyW",
   "metadata": {
    "id": "P54qdRGoRgyW"
   },
   "source": [
    "#PREDICTION on custom image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DX5wzLLCXMcg",
   "metadata": {
    "id": "DX5wzLLCXMcg"
   },
   "outputs": [],
   "source": [
    "img_tensor=next(iter(train_dataloader))[0][25]\n",
    "print(img_tensor.shape)\n",
    "\n",
    "plt.imshow(img_tensor.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KFGi439ORV7X",
   "metadata": {
    "id": "KFGi439ORV7X"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "image = Image.open('horse.png')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561c691",
   "metadata": {
    "id": "3561c691"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE))\n",
    "])\n",
    "  \n",
    "img_tensor = transform(image)\n",
    "print(img_tensor.shape)\n",
    "\n",
    "plt.imshow(img_tensor.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GD1wZsrHR2Is",
   "metadata": {
    "id": "GD1wZsrHR2Is"
   },
   "outputs": [],
   "source": [
    "def predict(model=vit,img_tensor=img_tensor):\n",
    "    X=img_tensor.unsqueeze(0)\n",
    "    X=X.to(torch.float32)\n",
    "    X=X.to(device)\n",
    "    print(X.shape)\n",
    "    y=model(X)\n",
    "    print(y)\n",
    "    print(train_dataset.classes[torch.argmax(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2krzJPFZSxiD",
   "metadata": {
    "id": "2krzJPFZSxiD"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "predict(img_tensor=img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G4hLdSqwTpsD",
   "metadata": {
    "id": "G4hLdSqwTpsD"
   },
   "outputs": [],
   "source": [
    "print(train_dataset.classes)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3ac2b88b"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "100a1d795a804d7a87296be75151fc5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "314ceb5ac10a4abcbac750b5efd6f1a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41012c8b07a74a2eb110a75fae49246a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4880fff204524b73b92a64694825abd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6728a139fd9b4e7c9a5e1adf41bfa33d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67466fcb2a8e4836a8a2e5760cde277a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_763028a4d9d14e988724aa003120362e",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4880fff204524b73b92a64694825abd2",
      "value": 170498071
     }
    },
    "763028a4d9d14e988724aa003120362e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a96f8e1477f2497a8fce5dd8be923415": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f195c48d67144d4591a94a700f99abcd",
       "IPY_MODEL_67466fcb2a8e4836a8a2e5760cde277a",
       "IPY_MODEL_b76bff5e97db44069aeb0c22358721c4"
      ],
      "layout": "IPY_MODEL_41012c8b07a74a2eb110a75fae49246a"
     }
    },
    "b76bff5e97db44069aeb0c22358721c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_100a1d795a804d7a87296be75151fc5a",
      "placeholder": "​",
      "style": "IPY_MODEL_fdbbe0a2fc764113afc69a1e9da3b284",
      "value": " 170498071/170498071 [00:05&lt;00:00, 30230175.99it/s]"
     }
    },
    "f195c48d67144d4591a94a700f99abcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_314ceb5ac10a4abcbac750b5efd6f1a8",
      "placeholder": "​",
      "style": "IPY_MODEL_6728a139fd9b4e7c9a5e1adf41bfa33d",
      "value": "100%"
     }
    },
    "fdbbe0a2fc764113afc69a1e9da3b284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
